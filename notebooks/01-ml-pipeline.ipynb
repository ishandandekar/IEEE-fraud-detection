{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting fraudulent transactions\n",
    "The aim of the project is to build a machine learning model to predict whether a transaction is fraudulent based on transaction and identity features.  \n",
    "Predicting fraudulent transactions is important to protect customers by preventing such transactions from taking place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Union, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transaction_cols = ['TransactionID', 'C3', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V27', 'V28', 'V32', 'V98', 'V116', 'V117', 'V118', 'V119', 'V120', 'V153', 'V154', 'V157', 'V158', 'V235', 'V284', 'V286', 'V297', 'V300', 'V301', 'V302', 'V303', 'V304', 'V305', 'V325', 'V327', 'V328', 'TransactionDT', 'TransactionAmt', 'C1', 'C2', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14', 'V95', 'V96', 'V97', 'V99', 'V100', 'V101', 'V102', 'V126', 'V127', 'V145', 'V166', 'V279', 'V280', 'V285', 'V287', 'V290', 'V291', 'V292', 'V293', 'V294', 'V295', 'V298', 'V299', 'V306', 'V307', 'V308', 'V309', 'V310', 'V311', 'V312', 'V313', 'V316', 'V317', 'V318', 'V319', 'V320', 'V321', 'V322', 'V323', 'V324', 'V326', 'V329', 'V330', 'V331', 'V332', 'V333', 'V334', 'V335', 'V336', 'V337', 'V338', 'V339', 'R_emaildomain', 'card1', 'card2', 'card3', 'card5', 'addr1', 'addr2', 'ProductCD', 'isFraud']\n",
    "\n",
    "train_identity_cols = ['TransactionID', 'id_08', 'id_13', 'id_17', 'id_19', 'id_20', 'id_21', 'id_26', 'id_16', 'id_27', 'DeviceInfo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data\n",
    "train_transaction_df = pd.read_csv(\"../data/train_transaction.csv\", usecols=train_transaction_cols)\n",
    "train_identity_df = pd.read_csv(\"../data/train_identity.csv\", usecols=train_identity_cols)\n",
    "\n",
    "# Merging the data\n",
    "dataset = pd.merge(\n",
    "    train_transaction_df,\n",
    "    train_identity_df,\n",
    "    how=\"left\",\n",
    "    on=\"TransactionID\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>isFraud</th>\n",
       "      <th>TransactionDT</th>\n",
       "      <th>TransactionAmt</th>\n",
       "      <th>ProductCD</th>\n",
       "      <th>card1</th>\n",
       "      <th>card2</th>\n",
       "      <th>card3</th>\n",
       "      <th>card5</th>\n",
       "      <th>addr1</th>\n",
       "      <th>...</th>\n",
       "      <th>id_08</th>\n",
       "      <th>id_13</th>\n",
       "      <th>id_16</th>\n",
       "      <th>id_17</th>\n",
       "      <th>id_19</th>\n",
       "      <th>id_20</th>\n",
       "      <th>id_21</th>\n",
       "      <th>id_26</th>\n",
       "      <th>id_27</th>\n",
       "      <th>DeviceInfo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2987000</td>\n",
       "      <td>0</td>\n",
       "      <td>86400</td>\n",
       "      <td>68.5</td>\n",
       "      <td>W</td>\n",
       "      <td>13926</td>\n",
       "      <td>NaN</td>\n",
       "      <td>150.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>315.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2987001</td>\n",
       "      <td>0</td>\n",
       "      <td>86401</td>\n",
       "      <td>29.0</td>\n",
       "      <td>W</td>\n",
       "      <td>2755</td>\n",
       "      <td>404.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>325.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2987002</td>\n",
       "      <td>0</td>\n",
       "      <td>86469</td>\n",
       "      <td>59.0</td>\n",
       "      <td>W</td>\n",
       "      <td>4663</td>\n",
       "      <td>490.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>330.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2987003</td>\n",
       "      <td>0</td>\n",
       "      <td>86499</td>\n",
       "      <td>50.0</td>\n",
       "      <td>W</td>\n",
       "      <td>18132</td>\n",
       "      <td>567.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>476.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2987004</td>\n",
       "      <td>0</td>\n",
       "      <td>86506</td>\n",
       "      <td>50.0</td>\n",
       "      <td>H</td>\n",
       "      <td>4497</td>\n",
       "      <td>514.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NotFound</td>\n",
       "      <td>166.0</td>\n",
       "      <td>542.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SAMSUNG SM-G892A Build/NRD90M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 121 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   TransactionID  isFraud  TransactionDT  TransactionAmt ProductCD  card1  \\\n",
       "0        2987000        0          86400            68.5         W  13926   \n",
       "1        2987001        0          86401            29.0         W   2755   \n",
       "2        2987002        0          86469            59.0         W   4663   \n",
       "3        2987003        0          86499            50.0         W  18132   \n",
       "4        2987004        0          86506            50.0         H   4497   \n",
       "\n",
       "   card2  card3  card5  addr1  ...  id_08 id_13     id_16  id_17  id_19  \\\n",
       "0    NaN  150.0  142.0  315.0  ...    NaN   NaN       NaN    NaN    NaN   \n",
       "1  404.0  150.0  102.0  325.0  ...    NaN   NaN       NaN    NaN    NaN   \n",
       "2  490.0  150.0  166.0  330.0  ...    NaN   NaN       NaN    NaN    NaN   \n",
       "3  567.0  150.0  117.0  476.0  ...    NaN   NaN       NaN    NaN    NaN   \n",
       "4  514.0  150.0  102.0  420.0  ...    NaN   NaN  NotFound  166.0  542.0   \n",
       "\n",
       "   id_20  id_21  id_26  id_27                     DeviceInfo  \n",
       "0    NaN    NaN    NaN    NaN                            NaN  \n",
       "1    NaN    NaN    NaN    NaN                            NaN  \n",
       "2    NaN    NaN    NaN    NaN                            NaN  \n",
       "3    NaN    NaN    NaN    NaN                            NaN  \n",
       "4  144.0    NaN    NaN    NaN  SAMSUNG SM-G892A Build/NRD90M  \n",
       "\n",
       "[5 rows x 121 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview the data\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numeric features to object\n",
    "# to follow the documentation\n",
    "numeric_cat_cols = [\"card1\", \"card2\", \"card3\", \"card5\", \"addr1\", \"addr2\", \"id_13\",\n",
    "                    \"id_17\", \"id_19\", \"id_20\", \"id_21\", \"id_26\"]\n",
    "\n",
    "cat_cols = list(dataset.select_dtypes(include=['object']).columns)\n",
    "cat_cols += numeric_cat_cols\n",
    "\n",
    "dataset[cat_cols] = dataset[cat_cols].astype(\"O\")\n",
    "\n",
    "# collecting all of the numerical features\n",
    "num_cols = [x for x in dataset.select_dtypes(include=['number']).columns\n",
    "            if x not in cat_cols\n",
    "            if x not in \"isFraud\"]\n",
    "\n",
    "# collecting all of the features with missing values\n",
    "features_with_na = [x for x in dataset.columns if dataset[x].isnull().sum() > 0]\n",
    "\n",
    "# determine percentage of missing values (expressed as decimals)\n",
    "# and display the result ordered by % of missin data\n",
    "missing_vals_df = pd.DataFrame(\n",
    "    dataset[features_with_na].isnull().mean().sort_values(ascending=False),\n",
    "    columns=[\"percentage\"]\n",
    ")\n",
    "\n",
    "# seperating features with missing\n",
    "# values to categorical and numeric\n",
    "cat_feat_with_na = [x for x in cat_cols if x in features_with_na]\n",
    "num_feat_with_na = [x for x in num_cols if x in features_with_na]\n",
    "\n",
    "# features to remove\n",
    "drop_features = [col for col in\n",
    "                 list(missing_vals_df.iloc[np.where(missing_vals_df.loc[cat_feat_with_na, 'percentage'] > 0.5)].index)]\n",
    "\n",
    "drop_features += [col for col in\n",
    "                  list(missing_vals_df.iloc[np.where(missing_vals_df.loc[num_feat_with_na, 'percentage'] > 0.5)].index)]\n",
    "\n",
    "# defining the different features\n",
    "discrete_features = [x for x in num_cols\n",
    "                     if len(dataset[x].unique()) < 20\n",
    "                     and x not in drop_features]\n",
    "\n",
    "continuous_features = [x for x in num_cols\n",
    "                       if x not in discrete_features + drop_features + [\"TransactionID\"]]\n",
    "\n",
    "high_cardinality_cats = [\"R_emaildomain\", \"card1\", \"card2\", \"card3\",\n",
    "                         \"card5\", \"addr1\", \"addr2\", \"id_13\", \"id_17\", \"id_19\", \"id_20\",\n",
    "                         \"id_21\", \"id_26\"]\n",
    "\n",
    "categorical_features = [x for x in cat_cols if x not in drop_features + high_cardinality_cats]\n",
    "\n",
    "all_features = discrete_features + continuous_features + high_cardinality_cats + categorical_features\n",
    "\n",
    "impute_freq = high_cardinality_cats + categorical_features + discrete_features\n",
    "\n",
    "cat_codes_cols = high_cardinality_cats + categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from typing import List, Union\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "class AggregateCategorical(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Reduces the cardinality of categorical features\n",
    "    Credit: Raj Sangani- https://bit.ly/3BSxdTX\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    features: str or list\n",
    "        The feature(s) with high cardinality that we want to\n",
    "        aggregate\n",
    "\n",
    "    threshold: float\n",
    "\n",
    "    Methods\n",
    "    ----------\n",
    "    fit:\n",
    "        The transformer will not learn from any parameter\n",
    "\n",
    "    transform:\n",
    "        Drops the explicitly selected features\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, features: List[Union[str, int]], threshold: float = 0.75):\n",
    "        if not isinstance(features, list) or len(features) == 0:\n",
    "            raise ValueError(\"Was expecting a list of features\")\n",
    "        self.features = features\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series = None):\n",
    "        threshold_value = int(self.threshold * len(X))\n",
    "        df = X.copy()\n",
    "\n",
    "        self.agg_values_ = defaultdict(list)\n",
    "        for col in self.features:\n",
    "            counts = Counter(df[col])\n",
    "            s = 0\n",
    "            # Loop through the category name and its corresponding frequency\n",
    "            for i, j in counts.most_common():\n",
    "                s += counts[i]\n",
    "                self.agg_values_[col].append(i)\n",
    "\n",
    "                if s >= threshold_value:\n",
    "                    break\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame):\n",
    "        df = X.copy()\n",
    "        for col in self.features:\n",
    "            df[col] = df[col].apply(lambda x: x if x in self.agg_values_[col] else \"other\")\n",
    "        return df\n",
    "\n",
    "\n",
    "class MostFrequentImputer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    A wrapper around `SimpleImputer` to return data frames with columns.\n",
    "    Credit: https://bit.ly/3r2N40k\n",
    "    \"\"\"\n",
    "    def __init__(self, features):\n",
    "        if not isinstance(features, list) or len(features) == 0:\n",
    "            raise ValueError(\"Was expecting a list of features\")\n",
    "\n",
    "        self.features = features\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.imputer_dict_ = {}\n",
    "        for feature in self.features:\n",
    "            self.imputer_dict_[feature] = X[feature].mode()[0]\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for feature in self.features:\n",
    "            X[feature] = X[feature].fillna(self.imputer_dict_[feature])\n",
    "        return X\n",
    "\n",
    "class MeanImputer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Numerical missing value imputer.\n",
    "    Credit: https://bit.ly/3r2N40k\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, features):\n",
    "        if not isinstance(features, list) or len(features) == 0:\n",
    "            raise ValueError(\"Was expecting a list of features\")\n",
    "\n",
    "        self.features = features\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # persist mode in a dictionary\n",
    "        self.imputer_dict_ = {}\n",
    "        for feature in self.features:\n",
    "            self.imputer_dict_[feature] = X[feature].mean()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for feature in self.features:\n",
    "            X[feature].fillna(self.imputer_dict_[feature], inplace=True)\n",
    "        return X\n",
    "\n",
    "class CategoryConverter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, features):\n",
    "        if not isinstance(features, list) or len(features) == 0:\n",
    "            raise ValueError(\"Was expecting a list of features\")\n",
    "\n",
    "        self.features = features\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.codes_ = {}\n",
    "        for feature in self.features:\n",
    "            X[feature] = X[feature].astype(\"category\")\n",
    "            self.codes_[feature] = dict(zip(X[feature].values, X[feature].cat.codes))\n",
    "            self.codes_[feature].update({np.nan:-1})\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for feature in self.features:\n",
    "            for val in X[feature].unique():\n",
    "                if val not in self.codes_[feature]:\n",
    "                    X[feature].replace(str(feature), -1, inplace=True)\n",
    "            X[feature] = X[feature].map(self.codes_[feature])\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train ROC_AUC:1.0\n",
      "test ROC_AUC: 0.9227074896773741\n"
     ]
    }
   ],
   "source": [
    "# Building a classifier without a pipeline\n",
    "\n",
    "df1_ = dataset.copy()\n",
    "\n",
    "# separate features and target\n",
    "X = df1_[all_features]\n",
    "y = df1_[\"isFraud\"]\n",
    "\n",
    "# Split data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Impute mode to high cardinality, categorical features,\n",
    "# and discrete features\n",
    "imp_most_features = MostFrequentImputer(features=impute_freq)\n",
    "imp_most_features.fit(X_train)\n",
    "X_train = imp_most_features.transform(X_train)\n",
    "X_test = imp_most_features.transform(X_test)\n",
    "\n",
    "# Aggreagate the high cardinality categoricals so they have less dimensions\n",
    "aggregate_categoricals = AggregateCategorical(high_cardinality_cats)\n",
    "aggregate_categoricals.fit(X_train)\n",
    "X_train = aggregate_categoricals.transform(X_train)\n",
    "X_test = aggregate_categoricals.transform(X_test)\n",
    "\n",
    "# Convert all categorical features to type category and use codes\n",
    "transform_dtypes = CategoryConverter(features=cat_codes_cols)\n",
    "transform_dtypes.fit(X_train)\n",
    "X_train = transform_dtypes.transform(X_train)\n",
    "X_test = transform_dtypes.transform(X_test)\n",
    "\n",
    "# Impute the mean to the continous features\n",
    "imp_mean = MeanImputer(features=continuous_features)\n",
    "imp_mean.fit(X_train)\n",
    "X_train = imp_mean.transform(X_train)\n",
    "X_test = imp_mean.transform(X_test)\n",
    "\n",
    "# Modelling a random forest algorithm\n",
    "random_forest_model = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=42)\n",
    "random_forest_model.fit(X_train, y_train)\n",
    "\n",
    "train_preds = random_forest_model.predict_proba(X_train)\n",
    "y_preds = random_forest_model.predict_proba(X_test)\n",
    "\n",
    "preds_roc_auc_ = roc_auc_score(y_test, y_preds[:, 1])\n",
    "train_roc_auc_ = roc_auc_score(y_train, train_preds[:, 1])\n",
    "\n",
    "print(f\"train ROC_AUC:{train_roc_auc_}\\ntest ROC_AUC: {preds_roc_auc_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a classifier with `pipeline`\n",
    "\n",
    "fraud_detection_pipe = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"most_frequest_imputer\",\n",
    "            MostFrequentImputer(features=impute_freq)\n",
    "        ),\n",
    "        (\n",
    "            \"aggregate_high_cardinality_features\",\n",
    "            AggregateCategorical(features=high_cardinality_cats)\n",
    "        ),\n",
    "        (\n",
    "            \"get_categorical_codes\",\n",
    "            CategoryConverter(features=cat_codes_cols)\n",
    "        ),\n",
    "        (\n",
    "            \"mean_imputer\",\n",
    "            MeanImputer(features=continuous_features)\n",
    "        ),\n",
    "        (\n",
    "            \"random_forest_clf\",\n",
    "            RandomForestClassifier(\n",
    "                n_estimators=100,\n",
    "                n_jobs=-1,\n",
    "                random_state=42\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train ROC_AUC:1.0\n",
      "test ROC_AUC: 0.9227074896773741\n"
     ]
    }
   ],
   "source": [
    "df2_ = dataset.copy()\n",
    "\n",
    "# seperate training features and target features\n",
    "# perform all processing to training features\n",
    "X = df2_[all_features]\n",
    "y = df2_[\"isFraud\"]\n",
    "\n",
    "# split into train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "fraud_detection_pipe.fit(X_train, y_train)\n",
    "\n",
    "# modelling\n",
    "train_preds = fraud_detection_pipe.predict_proba(X_train)\n",
    "y_preds = fraud_detection_pipe.predict_proba(X_test)\n",
    "\n",
    "preds_roc_auc_ = roc_auc_score(y_test, y_preds[:, 1])\n",
    "train_roc_auc_ = roc_auc_score(y_train, train_preds[:, 1])\n",
    "\n",
    "print(f\"train ROC_AUC:{train_roc_auc_}\\ntest ROC_AUC: {preds_roc_auc_}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63e8cd310c4b93056b905a09d43e9a591ee099a8fda9d1514e0998e9270c9e69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
